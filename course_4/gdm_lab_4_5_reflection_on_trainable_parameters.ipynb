{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_mtwaEDNtey"
      },
      "source": [
        "> <p><small><small>This Notebook is made available subject to the licence and terms set out in the <a href = \"http://www.github.com/google-deepmind/ai-foundations\">AI Research Foundations Github README file</a>."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJFpe_KMNzg1"
      },
      "source": [
        "![](https://storage.googleapis.com/dm-educational/assets/ai_foundations/GDM-Labs-banner-image-C4-white-bg.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CChJgNWQDIHN"
      },
      "source": [
        "# Lab: Trainable Parameters in the Transformer Model\n",
        "\n",
        "<a href='https://colab.research.google.com/github/google-deepmind/ai-foundations/blob/master/course_4/gdm_lab_4_5_reflection_on_trainable_parameters.ipynb' target='_parent'><img src='https://colab.research.google.com/assets/colab-badge.svg' alt='Open In Colab'/></a>\n",
        "\n",
        "Explore how individual neural network components can be combined to assemble a full transformer model.\n",
        "\n",
        "25 minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M7fZa-AxFuRl"
      },
      "source": [
        "## Overview\n",
        "\n",
        "In this lab, you will walk through a **full transformer implementation** in Keras and JAX, one component at a time. As you go through the implementation, you will define functions that compute the number of trainable parameters for each component to finally compute the number of trainable parameters of the entire transformer model.\n",
        "\n",
        "This lab provides you with the opportunity to explore all of the details of a full transformer architecture. It further illustrates an important property of implementations of complex neural network models, namely **modularity**. By breaking up a complex architecture into smaller building blocks and then combining them, they become a lot more manageable and it is easy to adjust individual components."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lWtLUxc0AMLP"
      },
      "source": [
        "### What you will learn\n",
        "By the end of this lab, you will::\n",
        "\n",
        "- Understand which neural network components are required to assemble a transformer model.\n",
        "- Understand how many trainable parameters each of these components has.\n",
        "- Understand how hyperparameters such as the number of transformer blocks or the vocabulary size affect the overall number of model parameters.\n",
        "\n",
        "### Tasks\n",
        "\n",
        "In this lab, you will:\n",
        "\n",
        "* Examine the following neural network components for assembling a transformer model:\n",
        "    1. Layer normalization\n",
        "    2. Embedding layer\n",
        "    3. Multi-head attention\n",
        "    4. Multi-layer perceptron\n",
        "    5. Transformer block\n",
        "    6. Output layer\n",
        "    7. Full transformer model\n",
        "* Implement a function that computes the number of trainable parameters for each of the components to gain a deeper understanding of each component.\n",
        "* Modify the model's hyperparameters to see how changes in the size of different components impact the total parameter count.\n",
        "* *(Optional)*: Train the model that is implemented in this lab on the **Africa Galore** dataset to verify that the implementation is working as expected.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96w3epfLDaKF"
      },
      "source": [
        "## How to use Google Colaboratory (Colab)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9MOwcoapD375"
      },
      "source": [
        "Google Colaboratory (also known as Google Colab) is a platform that allows you to run Python code in your browser. The code is written in **cells** that are executed on a remote server.\n",
        "\n",
        "To run a cell, hover over the cell and click on the `run` button to its left. The run button is the circle with the triangle (▶). Alternatively, you can also click on a cell and use the keyboard combination Ctrl+Return (or ⌘+Return if you are using a Mac).\n",
        "\n",
        "To try this out, run the following cell. This should print today's day of the week below it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVQKYr8bD4lf"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "print(f\"Today is {datetime.today():%A}.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uq6lF_ptD8a0"
      },
      "source": [
        "Note that the *order in which you run the cells matters*. When you are working through a lab, make sure to always run *all* cells in order, otherwise the code might not work. If you take a break while working on a lab, Colab may disconnect you and in that case, you have to execute all cells again before  continuing your work. To make this easier, you can select the cell you are currently working on and then choose __Runtime → Run before__  from the menu above (or use the keyboard combination Ctrl/⌘ + F8). This will re-execute all cells before the current one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3isa1CDJEBDW"
      },
      "source": [
        "## Imports\n",
        "\n",
        "In this lab, you will mainly implement functions that perform simple computations and do not require any additional packages. The transformer model that is already implemented in this lab uses the Keras and JAX packages, and you will also use methods from the custom `ai_foundation` package to verify your implementation.\n",
        "\n",
        "Run the following cell to import the required packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jTZuDEdt6GGM"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "\n",
        "import os # For setting Keras parameters.\n",
        "os.environ[\"KERAS_BACKEND\"] = \"jax\"\n",
        "\n",
        "# Install the custom package for this course.\n",
        "!pip install \"git+https://github.com/google-deepmind/ai-foundations.git@main\"\n",
        "\n",
        "import jax # For working with vectors and matrices.\n",
        "import jax.numpy as jnp # For working with vectors and matrices.\n",
        "import pandas as pd # For loading the dataset.\n",
        "import keras # For defining the transformer model.\n",
        "import tqdm # For displaying progress bars.\n",
        "from keras import layers # For defining the transformer model.\n",
        "from ai_foundations import training # For training your transformer model.\n",
        "from ai_foundations import generation # For prompting your transformer model.\n",
        "from ai_foundations import tokenization # For loading the BPE tokenizer.\n",
        "# For providing feedback.\n",
        "from ai_foundations.feedback.course_4 import counting_parameters as feedback"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCUdSNrsnB4n"
      },
      "source": [
        "## Components of the transformer\n",
        "\n",
        "The following cells implement all the components of the transformer model in Keras. Walk through each component and at the end of each component implement the function that computes how many parameters are being trained as part of this component."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zlBN9hwnZJ4"
      },
      "source": [
        "### Layer normalization\n",
        "\n",
        "The cell below implements the layer normalization component using Keras and JAX. Recall that the formula for layer normalization is:\n",
        "\n",
        "$$ \\mbox{LayerNorm}(\\mathbf{x}) = \\gamma \\frac{\\mathbf{x} - \\mu(\\mathbf{x})}{\\sqrt{\\mbox{Var}(\\mathbf{x}) + \\epsilon} } + \\beta,$$\n",
        "\n",
        "where $\\mu(\\mathbf{x})$ is the mean of elements in $\\mathbf{x}$ and $\\mbox{Var}(\\mathbf{x})$ is the variance of the elements in $\\mathbf{x}$. $\\gamma$ (gamma) and $\\beta$ (beta) are learnable parameters.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **ℹ️ Info: Keras layers**\n",
        ">\n",
        "> To understand how individual neural network components can be implemented in Keras, note that all Keras layers consist of an `__init__` and a `call` method.\n",
        ">\n",
        "> **Initialization method (`__init__`)**:\n",
        ">\n",
        ">The initialization function is called with the options for the respective component. For example, in the case of layer normalization, the two options are the embedding size `embedding_dim` and the `epsilon` argument, which defines the constant that is added to the variance to avoid division by 0 errors.\n",
        ">\n",
        ">Using the `embedding_dim` argument, the method initializes the $\\gamma$ and $\\beta$ vectors, which are the main parameters of this model.\n",
        ">\n",
        ">**`call` method**:\n",
        ">\n",
        ">The `call` method (also known as the forward function) is called whenever the model makes a prediction. Either during training to compute the loss function or during validation and testing to make predictions on data points that are not part of the training data. It defines how the input is transformed to the output of the component by using the model's parameters.\n",
        ">\n",
        ">Consider now the `call` function below. It is called with the input to the component `x`. It then outputs the layer normalized inputs.\n",
        ">\n",
        ">This function performs five steps:\n",
        ">1. It computes the mean $\\mu(\\mathbf{x})$ across all features using `jnp.mean`. Note that the `axis` argument is set to `-1`. This tells the `jnp.mean` function that the mean should be computed across the last dimension of `x` which is again, by convention, all features of a single data point.\n",
        ">2. It computes the variance across all features using the formula $\\mbox{Var}(\\mathbf{x}) = \\frac{1}{d_x}\\left(\\mathbf{x}-\\mu(\\mathbf{x})\\right)^2$, where $d_x$ is the dimension of `x`.\n",
        ">3. It computes the normalized values of `x` using the mean and variance.\n",
        ">4. It returns the normalized values combined with the scaling factor $\\gamma$ and the shifting term $\\beta$.\n",
        ">\n",
        "------\n",
        "\n",
        "<br />\n",
        "\n",
        "Run the following cell to define the `LayerNorm` component. Then walk through the code line by line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJsN1ySAvYH3"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(layers.Layer):\n",
        "    \"\"\"A Keras implementation of Layer Normalization.\n",
        "\n",
        "    This layer normalizes the activations of the previous layer for\n",
        "    each given example in a batch independently, across the features dimension.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: The dimension of the output of the attention mechanism.\n",
        "      epsilon: A small float added to variance to avoid dividing by zero.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, embedding_dim: int, epsilon: float = 1e-6, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.epsilon = epsilon\n",
        "\n",
        "        # The shape of the parameters (gamma and beta) is the size of the\n",
        "        # input embeddings and the size of the output of the attention mechanism\n",
        "        # `embedding_dim`.\n",
        "        shape = (embedding_dim,)\n",
        "\n",
        "        # Initialize gamma (scale) as a vector of ones.\n",
        "        self.gamma = self.add_weight(\n",
        "            shape=shape,\n",
        "            initializer='ones',\n",
        "            name='gamma'\n",
        "        )\n",
        "        # Initialize beta (shift) as a vector of zeros.\n",
        "        self.beta = self.add_weight(\n",
        "            shape=shape,\n",
        "            initializer='zeros',\n",
        "            name='beta'\n",
        "        )\n",
        "\n",
        "    def call(self, x: jax.Array):\n",
        "        \"\"\"\n",
        "        Applies the layer normalization logic.\n",
        "\n",
        "        Args:\n",
        "          x: The input tensor.\n",
        "            Shape: (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "          The normalized and transformed tensor.\n",
        "            Shape: (batch_size, sequence_length, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Calculate mean and variance over the feature axis (-1).\n",
        "        mean = jnp.mean(x, axis=-1, keepdims=True)\n",
        "        variance = jnp.mean(jnp.square(x - mean), axis=-1, keepdims=True)\n",
        "\n",
        "        # Normalize the input.\n",
        "        normalized_x = (x - mean) / jnp.sqrt(variance + self.epsilon)\n",
        "\n",
        "        # Apply the learned scale (gamma) and shift (beta).\n",
        "        return self.gamma * normalized_x + self.beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJAkTB2yyTQK"
      },
      "source": [
        "How many trainable parameters does this component have? Here, and throughout this lab, assume that the model has been initialized with the following hyperparameters:\n",
        "\n",
        "```python\n",
        "{\n",
        "    # The maximum number of input tokens in a sequence.\n",
        "    \"max_length\": 128,\n",
        "    # The dimension of the input embeddings, of the outputs of the\n",
        "    # attention mechanism, and of the output of every transformer block.\n",
        "    \"embedding_dim\": 256,\n",
        "    # The dimension of the hidden MLP layer in each transformer block.\n",
        "    \"mlp_dim\": 384,\n",
        "    # The number of attention heads.\n",
        "    \"num_heads\": 4,\n",
        "    # The number of transformer blocks.\n",
        "    \"num_blocks\": 2,\n",
        "    # The number of unique tokens in the vocabulary.\n",
        "    \"vocabulary_size\": 262144\n",
        "}\n",
        "```\n",
        "\n",
        "The following cell defines these parameters and a function that computes the number of trainable parameters based on the model hyperparameters. To get you started, this cell is already complete. For the remaining components in this lab, you will have to complete the function for computing the trainable parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "80bqMTKW1qCC"
      },
      "outputs": [],
      "source": [
        "# Define the model hyperparameters.\n",
        "MODEL_HYPERPARAMETERS = {\n",
        "    \"max_length\": 128,\n",
        "    \"embedding_dim\": 256,\n",
        "    \"mlp_dim\": 384,\n",
        "    \"num_heads\": 4,\n",
        "    \"num_blocks\": 2,\n",
        "    \"vocabulary_size\": 262144\n",
        "}\n",
        "\n",
        "\n",
        "def parameter_count_layer_norm(hyperparams: dict[str, int]) -> int:\n",
        "    embedding_dim =  hyperparams[\"embedding_dim\"]\n",
        "    parameter_count = embedding_dim + embedding_dim\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_layer_norm(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for layer normalization: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "opJcnHNvq8DJ"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_layer_norm`\n",
        "feedback.test_parameter_count_layer_norm(parameter_count_layer_norm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UZxgF8Ps23-R"
      },
      "source": [
        "### Coding Activity 1: Token and position embeddings\n",
        "\n",
        "The following cell implements both the sinusoidal positional embeddings and\n",
        "a component to embed the input tokens. The sinusoidal positional embeddings do not contain any trainable parameters, so you may ignore the `positional_encoding` function.\n",
        "\n",
        "<br />\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        "> Walk through the `TokenAndPositionEmbedding` implementation and then complete the `parameter_count_embedding` function to compute the number of trainable parameters in this component.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btcYR2b_3-aL"
      },
      "outputs": [],
      "source": [
        "class TokenAndPositionEmbedding(layers.Layer):\n",
        "    \"\"\"\n",
        "    A Keras layer that combines token and sinusoidal positional embeddings.\n",
        "\n",
        "    Args:\n",
        "      vocabulary_size: The size of the vocabulary.\n",
        "      embedding_dim: The dimensionality of the embeddings. Must be even.\n",
        "      max_length: The maximum length of the input sequences in tokens.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocabulary_size: int,\n",
        "        embedding_dim: int,\n",
        "        max_length: int,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.max_length = max_length\n",
        "        self.pos_encoding = self._positional_encoding()\n",
        "\n",
        "        self.token_embeddings = layers.Embedding(\n",
        "            input_dim=vocabulary_size, output_dim=embedding_dim, mask_zero=True\n",
        "        )\n",
        "\n",
        "    def _positional_encoding(self) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Creates a fixed sinusoidal positional encoding matrix.\n",
        "\n",
        "        This function generates a unique positional representation for each\n",
        "        token in a sequence using sine and cosine functions of different\n",
        "        frequencies.\n",
        "\n",
        "        Returns:\n",
        "          A JAX array of shape (1, max_length, embedding_dim) for the positional\n",
        "            encoding.\n",
        "        \"\"\"\n",
        "        depth = self.embedding_dim // 2\n",
        "        # Shape: (max_length, 1).\n",
        "        positions = jnp.arange(self.max_length)[:, jnp.newaxis]\n",
        "        depths = jnp.arange(depth)[jnp.newaxis, :] / depth\n",
        "        # Shape: (1, depth).\n",
        "        angle_rates = 1 / (10000**depths)\n",
        "        # Shape: (1, depth).\n",
        "        angle_rads = positions * angle_rates\n",
        "        # Shape: (max_length, depth).\n",
        "        pos_encoding = jnp.concatenate(\n",
        "            [jnp.sin(angle_rads), jnp.cos(angle_rads)], axis=-1\n",
        "        )\n",
        "        # Add a batch dimension for broadcasting.\n",
        "        return pos_encoding[jnp.newaxis, :, :]\n",
        "\n",
        "    def call(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Applies the embedding layer.\n",
        "\n",
        "        Args:\n",
        "          x: Input tensor of token IDs. Shape: (batch_size, sequence_length).\n",
        "\n",
        "        Returns:\n",
        "          A JAX array of shape (batch_size, sequence_length, embedding_dim)\n",
        "            representing the combined token and positional embeddings.\n",
        "        \"\"\"\n",
        "        # Get token embeddings from the lookup table.\n",
        "        # The input tensor `x` contains integer token IDs.\n",
        "        token_embeddings = self.token_embeddings(x)\n",
        "\n",
        "        # Scale token embeddings, as described in Vaswani et al., 2017.\n",
        "        token_embeddings *= jnp.sqrt(self.embedding_dim)\n",
        "\n",
        "        # Add the fixed positional embeddings.\n",
        "        # The positional encoding has shape (1, max_length, embedding_dim) and\n",
        "        # will be broadcasted across the batch dimension of token_embeddings.\n",
        "        return token_embeddings + self.pos_encoding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G9zx72P64v4P"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the number of parameters in the embedding layer:\n",
        "def parameter_count_embedding(hyperparams: dict[str, int]) -> int:\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_embedding(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for embedding component: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "Db_9Kkqx4KvQ"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_embedding`\n",
        "feedback.test_parameter_count_embedding(parameter_count_embedding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5uyDbTq6j4m"
      },
      "source": [
        "### Coding Activity 2: Multi-head attention\n",
        "\n",
        "The following cell implements the complete multi-head attention mechanism.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        ">Walk through this implementation and then complete the `parameter_count_attention` function.\n",
        ">\n",
        ">The `call` method here is complex since it supports highly parallelized computations where all computations for all attention heads and all examples in one batch are performed in parallel using 4-dimensional tensors (matrices with four dimensions). In order to estimate the number of trainable parameters, you only have to understand the first part of the `MultiHeadSelfAttention` and the `__init__` method. So it is okay if you are unsure what exactly is happening in the `call` method.\n",
        ">\n",
        ">\n",
        ">**Hints:**\n",
        ">\n",
        ">\n",
        ">- This component includes a layer normalization component. Use the `parameter_count_layer_norm` function from above to obtain the number of parameters for that component.\n",
        ">\n",
        ">- The queries, keys, and values are projected to a dimension of `embedding_dim / num_heads`. Since there are `num_heads` this means that the overall dimension of the projections across all heads is `embedding_dim / num_heads * num_heads = embedding_dim`. So for the purpose of computing the number of parameters, you may assume that there is only a single attention head that projects everything to a dimension `embedding_dim`.\n",
        ">\n",
        "> -  The implementation of `Dense` in Keras automatically adds bias terms. Therefore, if the input dimension that is passed to a Dense layer is $q$ and the output dimension is $r$ it will initialize a parameter matrix of dimension $q\\times r$ and a bias vector of dimension $r$. This results in total in $(q+1)\\times r$ parameters.\n",
        ">\n",
        "-----"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMhElgAz74qZ"
      },
      "outputs": [],
      "source": [
        "K_MASK = -2.3819763e38\n",
        "\n",
        "\n",
        "class MultiHeadSelfAttention(layers.Layer):\n",
        "    \"\"\"A Keras layer for multi-head self-attention with causal masking.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: The dimensionality of the embeddings. Must be divisible by\n",
        "        num_heads.\n",
        "      num_heads: The number of attention heads.\n",
        "      dropout_rate: The dropout rate to apply to the attention output.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        embedding_dim: int,\n",
        "        num_heads: int,\n",
        "        dropout_rate: float = 0.0,\n",
        "        **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout_rate = dropout_rate\n",
        "\n",
        "        # embedding_dim must be divisible by num_heads.\n",
        "        assert (\n",
        "            embedding_dim % num_heads == 0\n",
        "        ), \"embedding_dim must be divisible by num_heads\"\n",
        "        self.head_dim = embedding_dim // num_heads\n",
        "\n",
        "        # Define projection layers for query, key, value, and the final output.\n",
        "        self.q_dense = keras.layers.Dense(embedding_dim, name=\"q_projection\")\n",
        "        self.k_dense = keras.layers.Dense(embedding_dim, name=\"k_projection\")\n",
        "        self.v_dense = keras.layers.Dense(embedding_dim, name=\"v_projection\")\n",
        "        self.output_dense = keras.layers.Dense(embedding_dim,\n",
        "                                               name=\"output_projection\")\n",
        "\n",
        "        # Dropout layer.\n",
        "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
        "\n",
        "    def _split_heads(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"Splits the last dimension into (num_heads, head_dim) and transposes\n",
        "           the array such that the final shape is\n",
        "           (batch_size, num_heads, sequence_length, head_dim).\n",
        "\n",
        "        Args:\n",
        "          x: Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "          jax.Array of shape (batch_size, num_heads, sequence_length, head_dim).\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "        x = x.reshape(\n",
        "            batch_size, sequence_length, self.num_heads, self.head_dim\n",
        "        )\n",
        "        return x.transpose((0, 2, 1, 3))\n",
        "\n",
        "    def call(self, x: jax.Array, training: bool = False) -> jax.Array:\n",
        "        \"\"\"Forward pass for the MultiHeadSelfAttention layer.\n",
        "\n",
        "        Args:\n",
        "          x: Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "          training: Python boolean indicating whether the layer should behave in\n",
        "            training mode (apply dropout) or in inference mode.\n",
        "\n",
        "        Returns:\n",
        "          The output tensor of shape\n",
        "            (batch_size, sequence_length, embedding_dim).\n",
        "        \"\"\"\n",
        "        batch_size, sequence_length, _ = x.shape\n",
        "\n",
        "        # Project inputs to Q, K, V.\n",
        "        query = self.q_dense(x)  # (batch_size, sequence_length, embedding_dim).\n",
        "        key = self.k_dense(x)  # (batch_size, sequence_length, embedding_dim).\n",
        "        value = self.v_dense(x)  # (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        # Reshape for multi-head attention.\n",
        "\n",
        "        # (batch, num_heads, sequence_length, head_dim).\n",
        "        query = self._split_heads(query)\n",
        "\n",
        "        # (batch, num_heads, sequence_length, head_dim).\n",
        "        key = self._split_heads(key)\n",
        "\n",
        "        # (batch, num_heads, sequence_length, head_dim).\n",
        "        value = self._split_heads(value)\n",
        "\n",
        "        # Scaled dot-product attention.\n",
        "        # Matrix multiplication scores:\n",
        "        # (..., sequence_length, d_k) x (..., d_k, sequence_length)\n",
        "        #    -> (..., sequence_length, sequence_length).\n",
        "        logits_raw = jnp.matmul(query, key.transpose((0, 1, 3, 2)))\n",
        "        logits_raw /= jnp.sqrt(self.head_dim)\n",
        "\n",
        "        # Apply causal (look-ahead) mask.\n",
        "        # The mask ensures that attention is only applied to previous positions.\n",
        "        causal_mask = jnp.tril(jnp.ones((sequence_length, sequence_length),\n",
        "                                        dtype=bool))\n",
        "        logits_masked = jnp.where(causal_mask, logits_raw, K_MASK)\n",
        "\n",
        "        # Apply softmax to get attention weights.\n",
        "        # (batch, num_heads, sequence_length, sequence_length).\n",
        "        attention_weights = jax.nn.softmax(logits_masked, axis=-1)\n",
        "\n",
        "        # Apply attention weights to values.\n",
        "        # (batch, num_heads, sequence_length, head_dim).\n",
        "        attention_output = jnp.matmul(attention_weights, value)\n",
        "\n",
        "        # Concatenate heads and apply final projection.\n",
        "        # Transpose back to (batch_size, sequence_length, num_heads, head_dim).\n",
        "        attention_output = attention_output.transpose((0, 2, 1, 3))\n",
        "\n",
        "        # Reshape to (batch_size, sequence_length, embedding_dim).\n",
        "        attention_output = attention_output.reshape(\n",
        "            batch_size, sequence_length, self.embedding_dim\n",
        "        )\n",
        "        attention_output = self.output_dense(attention_output)\n",
        "\n",
        "        # Apply dropout.\n",
        "        attention_output = self.dropout(attention_output, training=training)\n",
        "\n",
        "        return attention_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8pUVJmHF8j58"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the number of parameters in the multi-head\n",
        "# attention component here:\n",
        "def parameter_count_attention(hyperparams: dict[str, int]) -> int:\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_attention(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(\"Number of parameters for multi-head attention component:\"\n",
        "          f\" {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "un84cnLU42Sq"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_attention`\n",
        "feedback.test_parameter_count_attention(parameter_count_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QtGEoo73AQzX"
      },
      "source": [
        "### Coding Activity 3: MLP component\n",
        "\n",
        "The following cell implements the MLP component including layer normalization as part of the transformer block.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        ">Walk through this implementation and then complete the `parameter_count_mlp` function.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfsRQyKVAhz3"
      },
      "outputs": [],
      "source": [
        "class MultiLayerPerceptron(layers.Layer):\n",
        "    \"\"\"Multi-layer perceptron component.\n",
        "\n",
        "    This component implements a two-layer multi-layer perceptron. It introduces\n",
        "    a non-linearity and improves the model's ability to learn complex patterns.\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: The dimensionality of the embedding space.\n",
        "      mlp_dim: The dimensionality of the hidden layer in the MLP component\n",
        "        (often larger than embedding_dim).\n",
        "      dropout_rate: The dropout rate applied to the output of the MLP\n",
        "        component.\n",
        "      activation: The activation function used in the first dense layer.\n",
        "\n",
        "    Returns:\n",
        "      Output tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "        after applying the MLP.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 embedding_dim: int,\n",
        "                 mlp_dim: int,\n",
        "                 dropout_rate: float = 0.0,\n",
        "                 activation: str = \"relu\",\n",
        "                 **kwargs: dict):\n",
        "        super().__init__(**kwargs)\n",
        "        # Define a two-layer MLP.\n",
        "        self.mlp = keras.Sequential([\n",
        "            # Hidden layer.\n",
        "            layers.Dense(mlp_dim, activation=activation),\n",
        "            # Output layer.\n",
        "            layers.Dense(embedding_dim)\n",
        "        ])\n",
        "        self.dropout = layers.Dropout(dropout_rate)\n",
        "\n",
        "    def call(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"Applies the MLP to the input tensor.\n",
        "\n",
        "        Args:\n",
        "          x: Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "          Output tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "        \"\"\"\n",
        "        # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "        mlp_output = self.mlp(x)\n",
        "\n",
        "        # Apply dropout.\n",
        "        # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "        mlp_output = self.dropout(mlp_output)\n",
        "\n",
        "        return mlp_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u2U_cG3wbP5W"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the number of parameters in the MLP component\n",
        "# here:\n",
        "def parameter_count_mlp(hyperparams):\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_mlp(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for MLP component: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "_7hvlnS19zP1"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_mlp`\n",
        "feedback.test_parameter_count_mlp(parameter_count_mlp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4PZCF1hBeIeL"
      },
      "source": [
        "### Coding Activity 4: Transformer block\n",
        "\n",
        "The following cell implements the transformer block that first passes the input through the multi-head attention mechanism and then passes it through the MLP component.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        ">Walk through this implementation and then complete the `parameter_count_transformer_block` function.\n",
        ">\n",
        ">\n",
        ">**Hints:**\n",
        ">- This block includes both a multi-head attention component and an MLP component. Use your `parameter_count_attention` and `parameter_count_mlp` functions from above to compute the number of parameters for these components.\n",
        ">- This block includes layer normalization after both the multi-head attention component and the MLP component. Use your `parameter_count_layer_norm` function from above to compute the number of parameters for these components.\n",
        ">\n",
        "------\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VTVs_nABeHB1"
      },
      "outputs": [],
      "source": [
        "class TransformerBlock(layers.Layer):\n",
        "  \"\"\"A single transformer block.\n",
        "\n",
        "    The transformer block is a fundamental component of the transformer\n",
        "    architecture, which is commonly used for sequence-based tasks. It consists\n",
        "    of a MultiHeadAttention layer followed by an MLP,\n",
        "    with layer normalization and dropout applied at each step.\n",
        "\n",
        "    Example:\n",
        "      transformer_block = TransformerBlock(embedding_dim=256, num_heads=8,\n",
        "                                           mlp_dim=1024)\n",
        "      output = transformer_block(inputs)\n",
        "\n",
        "    Args:\n",
        "      embedding_dim: The dimensionality of the input embedding (also the output\n",
        "        size of the attention layer).\n",
        "      num_heads: The number of attention heads in the multi-head attention\n",
        "        mechanism.\n",
        "      mlp_dim: The number of units in the MLP.\n",
        "      dropout_rate: Dropout rate, between 0 and 1.\n",
        "      activation: The activation function to use in the MLP.\n",
        "    \"\"\"\n",
        "\n",
        "  def __init__(self,\n",
        "               embedding_dim: int,\n",
        "               num_heads: int,\n",
        "               mlp_dim: int,\n",
        "               dropout_rate: float = 0.0,\n",
        "               activation: str = \"relu\",\n",
        "               **kwargs: dict):\n",
        "    super().__init__(**kwargs)\n",
        "\n",
        "    self.self_attention = MultiHeadSelfAttention(embedding_dim,\n",
        "                                                 num_heads,\n",
        "                                                 dropout_rate)\n",
        "\n",
        "    self.layer_norm_attention = LayerNorm(embedding_dim)\n",
        "    self.feed_forward = MultiLayerPerceptron(embedding_dim,\n",
        "                                             mlp_dim,\n",
        "                                             dropout_rate,\n",
        "                                             activation)\n",
        "    self.layer_norm_mlp = LayerNorm(embedding_dim)\n",
        "\n",
        "  def call(self, x: jax.Array) -> jax.Array:\n",
        "    \"\"\"Applies a single transformer block to the input tensor.\n",
        "\n",
        "    Args:\n",
        "      x: The input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "    Returns:\n",
        "      The output tensor of shape (batch_size, sequence_length, embedding_dim)\n",
        "        after applying the transformer block.\n",
        "    \"\"\"\n",
        "    # Apply masked self-attention.\n",
        "    # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "    attention_output = self.self_attention(x)\n",
        "\n",
        "    # Add residual connection.\n",
        "    attention_output = attention_output + x\n",
        "\n",
        "    # Apply layer normalization.\n",
        "    attention_output = self.layer_norm_attention(attention_output)\n",
        "\n",
        "    # Multi-layer perceptron applied to attention output.\n",
        "    # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "    mlp_output = self.feed_forward(attention_output)\n",
        "\n",
        "    # Add residual connection.\n",
        "    # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "    mlp_output = mlp_output + attention_output\n",
        "\n",
        "    # Apply layer normalization.\n",
        "    # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "    mlp_output = self.layer_norm_mlp(mlp_output)\n",
        "\n",
        "    return mlp_output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tWtGFhmKhRDW"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the number of parameters in\n",
        "# one transformer block here:\n",
        "def parameter_count_transformer_block(hyperparams):\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_transformer_block(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for one transformer block: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "n87C1UaO-pRo"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_transformer_block`\n",
        "feedback.test_parameter_count_transformer_block(parameter_count_transformer_block)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhYvBAKrrGjH"
      },
      "source": [
        "### Coding Activity 5: The output layer\n",
        "\n",
        "The following cell implements the last component that is needed, namely the output layer with a SoftMax activation function. This layer outputs the probability distribution over the next token.\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        ">Walk through this implementation and then complete the `parameter_count_output_layer` function.\n",
        ">\n",
        "------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C7oXlpJYGGOu"
      },
      "outputs": [],
      "source": [
        "class OutputLayer(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A layer to compute the log probability distribution over the vocabulary.\n",
        "\n",
        "    This layer projects the input tensor to the vocabulary size and applies a\n",
        "    log-softmax activation.\n",
        "\n",
        "    Args:\n",
        "        vocabulary_size: The size of the vocabulary.\n",
        "    \"\"\"\n",
        "    def __init__(self, vocabulary_size: int, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.vocabulary_size = vocabulary_size\n",
        "\n",
        "        # The dense layer projects the input from embedding_dim to\n",
        "        # vocabulary_size.\n",
        "        self.output_layer = keras.layers.Dense(vocabulary_size,\n",
        "                                        name=\"output_projection\")\n",
        "\n",
        "    def call(self, x: jax.Array) -> jax.Array:\n",
        "        \"\"\"\n",
        "        Forward pass for the OutputLayer.\n",
        "\n",
        "        Args:\n",
        "          x: Input tensor of shape (batch_size, sequence_length, embedding_dim).\n",
        "\n",
        "        Returns:\n",
        "          The logits for each token in the vocabulary, with shape\n",
        "            (batch_size, sequence_length, vocabulary_size).\n",
        "        \"\"\"\n",
        "        # Project the embedding_dim dimension to the vocabulary size.\n",
        "        logits = self.output_layer(x)\n",
        "\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrY-DaEftGDc"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the number of parameters in\n",
        "# the output layer here:\n",
        "def parameter_count_output_layer(hyperparams):\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_output_layer(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for output layer: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "toy2znRf-4FT"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_output_layer`\n",
        "feedback.test_parameter_count_output_layer(parameter_count_output_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7sqZeF5jAI7"
      },
      "source": [
        "### Coding Activity 6: Putting it all together\n",
        "\n",
        "With all of the components in place, the following cell implements the final transformer model.\n",
        "\n",
        "It consists of three components:\n",
        "\n",
        "1. The input embedding layer\n",
        "2. A stack of `n_blocks` transformer blocks\n",
        "3. The output layer\n",
        "\n",
        "<br>\n",
        "\n",
        "------\n",
        "> **💻 Your task:**\n",
        ">\n",
        ">Walk through this implementation and then complete the `parameter_count_transformer` function.\n",
        ">\n",
        ">As above, use the existing functions to compute the number of parameters for the individual components.\n",
        ">\n",
        "------\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6uHVgSXi_OX"
      },
      "outputs": [],
      "source": [
        "class TransformerModel(layers.Layer):\n",
        "    \"\"\"Implements the full transformer model in Keras.\n",
        "\n",
        "    Args:\n",
        "      vocabulary_size: The size of the vocabulary, i.e., the number of unique\n",
        "        tokens.\n",
        "      max_length: The maximum length of the input sequences.\n",
        "      embedding_dim: The dimensionality of the embedding space.\n",
        "      mlp_dim: The number of units in the MLP of each transformer block.\n",
        "      num_heads:The number of attention heads in the multi-head attention\n",
        "        mechanism.\n",
        "      num_blocks: The number of transformer blocks to stack in the model.\n",
        "      dropout_rate: The dropout rate to prevent overfitting.\n",
        "      activation: The activation function to use in the MLP of each transformer\n",
        "        block.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 vocabulary_size: int,\n",
        "                 max_length: int,\n",
        "                 embedding_dim: int = 256,\n",
        "                 mlp_dim: int = 256,\n",
        "                 num_heads: int = 2,\n",
        "                 num_blocks: int = 1,\n",
        "                 dropout_rate: float = 0.0,\n",
        "                 activation: str = \"relu\",\n",
        "                 **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        # Create an embedding layer that combines token and positional\n",
        "        # embeddings.\n",
        "        self.embedding_layer = TokenAndPositionEmbedding(vocabulary_size,\n",
        "                                                         embedding_dim,\n",
        "                                                         max_length)\n",
        "\n",
        "        # Create a stack of transformer blocks.\n",
        "        self.transformer_blocks = keras.Sequential()\n",
        "        for _ in range(num_blocks):\n",
        "            self.transformer_blocks.add(TransformerBlock(\n",
        "                embedding_dim,\n",
        "                num_heads,\n",
        "                mlp_dim,\n",
        "                dropout_rate=dropout_rate,\n",
        "                activation=activation))\n",
        "\n",
        "        # Create output layer.\n",
        "        self.output_layer = OutputLayer(vocabulary_size)\n",
        "\n",
        "\n",
        "    def call(self, x: jax.Array) -> jax.Array:\n",
        "\n",
        "        # Embed input tokens.\n",
        "        # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "        x = self.embedding_layer(x)\n",
        "\n",
        "         # Shape: (batch_size, sequence_length, embedding_dim).\n",
        "        x = self.transformer_blocks(x)\n",
        "\n",
        "        # Compute output (log-probabilties for each token).\n",
        "         # Shape: (batch_size, sequence_length, vocabulary_size).\n",
        "        output = self.output_layer(x)\n",
        "\n",
        "        return output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8avefz2Zx6yb"
      },
      "outputs": [],
      "source": [
        "# Implement the computation of the total number of parameters in the\n",
        "# transformer here:\n",
        "def parameter_count_transformer(hyperparams):\n",
        "\n",
        "    parameter_count = ...\n",
        "\n",
        "    return parameter_count\n",
        "\n",
        "\n",
        "param_count = parameter_count_transformer(MODEL_HYPERPARAMETERS)\n",
        "if param_count != ...:\n",
        "    print(f\"Number of parameters for full transformer model: {param_count:,}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "TbvFHj8N_LAQ"
      },
      "outputs": [],
      "source": [
        "# @title Run this cell to check your implementation of `parameter_count_transformer`\n",
        "feedback.test_parameter_count_transformer(parameter_count_transformer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMxa_zHKDvP6"
      },
      "source": [
        "You have now walked through the full implementation of the transformer model by exploring each of its components and how they are combined.\n",
        "\n",
        "As you observed, this model has 135 million trainable parameters, which is quite a considerable number. For all of these parameters, the optimizer has to compute gradients for each example and then update the weights on each training step. This is why training usually takes a lot of time. Also note, in comparison to a model such as Gemma-1B, this model is still much smaller.\n",
        "\n",
        "Now go back through the number of parameters of each component. Which components introduce a lot of parameters? Which ones are less so?\n",
        "\n",
        "Then investigate which hyperparameters have a big effect on the number of parameters of the model. Edit the parameters below and observe how the parameters of each component change.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6h4KxwrMpkFT"
      },
      "outputs": [],
      "source": [
        "# @title Compute trainable parameters\n",
        "\n",
        "max_length = 128 # @param {\"type\": \"number\"}\n",
        "embedding_dim = 256 # @param {\"type\": \"number\"}\n",
        "mlp_dim = 384 # @param {\"type\": \"number\"}\n",
        "num_heads = 4 # @param {\"type\": \"number\"}\n",
        "num_blocks = 2 # @param {\"type\": \"number\"}\n",
        "vocabulary_size = 262144 # @param {\"type\": \"number\"}\n",
        "\n",
        "your_hyperparameters = {\n",
        "    \"max_length\": max_length,\n",
        "    \"embedding_dim\": embedding_dim,\n",
        "    \"mlp_dim\": mlp_dim,\n",
        "    \"num_heads\": num_heads,\n",
        "    \"num_blocks\": num_blocks,\n",
        "    \"vocabulary_size\": vocabulary_size\n",
        "}\n",
        "\n",
        "# Compute parameter counts.\n",
        "parameter_counts = {\n",
        "    \"embedding\": parameter_count_embedding(your_hyperparameters)\n",
        "}\n",
        "\n",
        "for i in range(your_hyperparameters[\"num_blocks\"]):\n",
        "    parameter_counts[f\"transformer_block_{i}\"] = parameter_count_transformer_block(your_hyperparameters)\n",
        "    parameter_counts[f\"  attention_{i}\"] = parameter_count_attention(your_hyperparameters)\n",
        "    parameter_counts[f\"  layer_norm_attention_{i}\"] = parameter_count_layer_norm(your_hyperparameters)\n",
        "    parameter_counts[f\"  mlp_{i}\"] = parameter_count_mlp(your_hyperparameters)\n",
        "    parameter_counts[f\"  layer_norm_mlp_{i}\"] = parameter_count_layer_norm(your_hyperparameters)\n",
        "\n",
        "parameter_counts[\"output_layer\"] = parameter_count_output_layer(your_hyperparameters)\n",
        "\n",
        "# Print the parameter counts.\n",
        "total = parameter_count_transformer(your_hyperparameters)\n",
        "max_key_width = len(max(parameter_counts.keys(), key=len))\n",
        "max_value_width = 80 - max_key_width\n",
        "\n",
        "separator_length = max_key_width + 2 + max_value_width\n",
        "print(\"Parameters of each component:\")\n",
        "separator_length = max_key_width + 2 + max_value_width\n",
        "print(\"-\" * separator_length)\n",
        "print(f\"{'Component':<{max_key_width}}  {'Parameters':>{max_value_width}}\")\n",
        "for key, value in parameter_counts.items():\n",
        "    if not key.startswith(\" \"):\n",
        "        print(\"-\" * separator_length)\n",
        "        print(f\"{key:<{max_key_width}}  {value:>{max_value_width},}\")\n",
        "        if key.startswith(\"transformer_block\"):\n",
        "            print(\"~\" * separator_length)\n",
        "    else:\n",
        "        print(f\"{key:<{max_key_width}}  {value:>{max_value_width},}\")\n",
        "\n",
        "print(\"-\" * separator_length)\n",
        "print(f\"{'Total':<{max_key_width}}  {total:>{max_value_width},}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnMk39y19xy5"
      },
      "source": [
        "## Optional: Training the model\n",
        "\n",
        "As a last optional exercise, if you would like to see this model in action, you can run the following hidden cell to load the Africa Galore dataset, tokenize and pad the data, and train the model. This will take about one minute to run on a Colab instance with a GPU or 10 minutes on a Colab instance with a CPU.\n",
        "\n",
        "You can then sample continuations to a prompt from the model in the cell after the training loop.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Hh6Z-d0p34H"
      },
      "source": [
        "### Load and tokenize the dataset\n",
        "\n",
        "Run the following cell to load the Africa Galore dataset and tokenize it with the Byte Pair Encoding tokenizer from the previous courses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hxQ7_vK8Vdcb"
      },
      "outputs": [],
      "source": [
        "# Load the dataset and the tokenizer.\n",
        "\n",
        "africa_galore = pd.read_json(\n",
        "    \"https://storage.googleapis.com/dm-educational/assets/ai_foundations/africa_galore.json\"\n",
        ")\n",
        "dataset = africa_galore[\"description\"].values\n",
        "print(\"Loaded dataset with\", dataset.shape[0], \"paragraphs.\")\n",
        "BPEWordTokenizer = tokenization.BPEWordTokenizer\n",
        "tokenizer = BPEWordTokenizer.from_url(\"https://storage.googleapis.com/dm-educational/assets/ai_foundations/bpe_tokenizer_3000_v2.pkl\")\n",
        "\n",
        "encoded_tokens = []\n",
        "for paragraph in tqdm.tqdm(dataset, unit=\"paragraphs\"):\n",
        "    encoded_tokens.append(tokenizer.encode(paragraph))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWBNti48qCcA"
      },
      "source": [
        "### Prepare the dataset for training\n",
        "\n",
        "Run the following cell to pad and truncate the paragraphs and prepare the input and target sequences for training your model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uzI3OwbmXKtQ"
      },
      "outputs": [],
      "source": [
        "max_length = 300\n",
        "padded_sequences = keras.preprocessing.sequence.pad_sequences(\n",
        "        encoded_tokens,\n",
        "        maxlen=max_length,\n",
        "        padding=\"post\",\n",
        "        truncating=\"post\",\n",
        "        value=tokenizer.pad_token_id,\n",
        "    )\n",
        "# Prepare input and target for the transformer model.\n",
        "# For each example, extract all tokens except the last one.\n",
        "input_sequences = padded_sequences[:, :-1]\n",
        "# For each example, extract all tokens except the first one.\n",
        "target_sequences = padded_sequences[:, 1:]\n",
        "\n",
        "max_length = input_sequences.shape[1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5TG3pvkqOGq"
      },
      "source": [
        "### Define and train the model\n",
        "\n",
        "The following cell defines the transformer model using the `TransformerModel` class that you defined above. It also initializes the optimizer (Adam) and the loss function (a multi-class cross-entropy loss), and attaches both of these training components to the model. Finally, it initializes a function that prints a generation after every tenth epoch which allows you to monitor the training progress."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bLcr6mmNXTIC"
      },
      "outputs": [],
      "source": [
        "# Set a seed for reproducability.\n",
        "keras.utils.set_random_seed(3112)\n",
        "\n",
        "# Initialize the transformer model that you defined above.\n",
        "transformer = TransformerModel(\n",
        "    tokenizer.vocabulary_size,\n",
        "    max_length=max_length,\n",
        "    dropout_rate=0.1,\n",
        "    num_blocks=2,\n",
        "    embedding_dim=64,\n",
        "    mlp_dim=128,\n",
        ")\n",
        "\n",
        "# Build the Keras model.\n",
        "input_layer = keras.Input((max_length,))\n",
        "output_layer = transformer(input_layer)\n",
        "model = keras.Model(input_layer, output_layer)\n",
        "\n",
        "# Initialize the optimizer.\n",
        "optimizer = keras.optimizers.Adam(learning_rate=2.5e-3)\n",
        "# Initialize the loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(\n",
        "    # The output layer outputs raw logits rather than probabilities computed\n",
        "    # through the softmax to improve efficiency and avoid very small numbers.\n",
        "    from_logits=True,\n",
        "    ignore_class=tokenizer.pad_token_id,\n",
        "    # Average the loss across the batch size.\n",
        "    reduction=\"sum_over_batch_size\",\n",
        ")\n",
        "\n",
        "# Attach the optimizer and loss function to the model.\n",
        "model.compile(optimizer=optimizer, loss=loss_fn)\n",
        "\n",
        "# Initialize a callback function that prints a generation after every 10 epochs.\n",
        "prompt = \"Jide\"\n",
        "prompt_ids = tokenizer.encode(prompt)\n",
        "text_gen_callback = training.TextGenerator(\n",
        "    max_tokens=11, start_tokens=prompt_ids, tokenizer=tokenizer, print_every=10\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvSv4dTUqp4y"
      },
      "source": [
        "Run the following cell to use the `fit` method to train your model for 100 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X9UlfefK0re"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    x=input_sequences,\n",
        "    y=target_sequences,\n",
        "    batch_size=32,\n",
        "    epochs=100,\n",
        "    callbacks=[text_gen_callback],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_7MdUNlSsOqS"
      },
      "source": [
        "### Prompt your model\n",
        "\n",
        "Finally, you can use the following cell to prompt your transformer model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i_DyGhpCljqV"
      },
      "outputs": [],
      "source": [
        "# Use this cell to generate new texts.\n",
        "# You can edit the prompt varible to modify the input prompt.\n",
        "prompt = \"Jide was thirsty so she went looking for a\"\n",
        "\n",
        "# Greedy sampling.\n",
        "\n",
        "generated_text, _ = generation.generate_text(\n",
        "    prompt,\n",
        "    n_tokens = 1,\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    sampling_mode=\"greedy\"\n",
        ")\n",
        "print(f\"Generated text: {generated_text}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVgRkSTvCA2J"
      },
      "source": [
        "## Summary\n",
        "\n",
        "In this activity you explored all the **individual components of the transformer model**. You have seen how they are implemented, how many trainable parameters they have, and how different model hyperparameters affect the number of trainable parameters.\n",
        "\n",
        "This lab has also illustrated how splitting up a model into its individual components makes it more manageable to implement and maintain the code for complex models. Instead of implementing every part of the transformer in one gigantic Keras model, you have seen that it is possible to combine smaller building blocks, each of which are less complex. This type of **modularity** is an important property of frameworks like Keras.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ss0HGXjueYhK"
      },
      "source": [
        "## Solutions\n",
        "\n",
        "The following cells provide reference solutions to the coding activities in this notebook. If you really get stuck after trying to solve the activities yourself, you may want to consult these solutions.\n",
        "\n",
        "It is recommended that you *only* look at the solutions after you have tried to solve the activities *multiple times*. The best way to learn challenging concepts in computer science and artificial intelligence is to debug your code piece-by-piece until it works, rather than copying existing solutions.\n",
        "\n",
        "If you feel stuck, you may want to first try to debug your code. For example, by adding additional print statements to see what your code is doing at every step. This will provide you with a much deeper understanding of the code and the materials. It will also provide you with practice on how to solve challenging coding problems beyond this course.\n",
        "\n",
        "To view the solutions for an activity, click on the arrow to the left of the activity name. If you consult the solutions, do not copy and paste them into the cells above. Instead, look at them, and type them manually into the cell. This will help you understand where you went wrong.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBLRvtNWo9AW"
      },
      "source": [
        "### Coding Activity 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1W9HAVuspH8Z"
      },
      "outputs": [],
      "source": [
        "def parameter_count_embedding(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for the token embedding matrix.\n",
        "\n",
        "    The embedding matrix has shape `vocabulary_size x embedding_dim`.\n",
        "\n",
        "    Args:\n",
        "      hyperparams: Model hyperparameters. Expects `\"vocabulary_size\"` and\n",
        "          `\"embedding_dim\"`.\n",
        "\n",
        "    Returns:\n",
        "        int: Total number of trainable parameters for the embedding layer.\n",
        "    \"\"\"\n",
        "\n",
        "    vocabulary_size = hyperparams[\"vocabulary_size\"]\n",
        "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
        "    # The embedding matrix is of size `vocabulary_size` x `embedding_dim`.\n",
        "    parameter_count = vocabulary_size * embedding_dim\n",
        "    return parameter_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BbNWHXOdpEMO"
      },
      "source": [
        "### Coding Activity 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O5dY-SAeafw"
      },
      "outputs": [],
      "source": [
        "def parameter_count_attention(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for a multi-head attention sublayer with LayerNorm.\n",
        "\n",
        "    Counts parameters for the query, key, value, and output linear projections,\n",
        "    each modeled as a dense layer with bias of shape\n",
        "    `embedding_dim x embedding_dim` plus a bias vector of size `embedding_dim`.\n",
        "\n",
        "    Args:\n",
        "      hyperparams: Model hyperparameters. Expects `\"embedding_dim\"`.\n",
        "\n",
        "    Returns:\n",
        "      Total number of trainable parameters for the attention sublayer.\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
        "    # Parameters for query projection.\n",
        "    # Note that for the key, query, and value projections, the first dimension\n",
        "    # is d_head * num_heads which happens to be embedding_dim.\n",
        "    q_parameter_count = (embedding_dim + 1) * embedding_dim\n",
        "    # Parameters for key projection.\n",
        "    k_parameter_count = (embedding_dim + 1) * embedding_dim\n",
        "\n",
        "    # Parameters for value projection.\n",
        "    v_parameter_count = (embedding_dim + 1) * embedding_dim\n",
        "\n",
        "    # Parameters for output projection.\n",
        "    o_parameter_count = (embedding_dim + 1) * embedding_dim\n",
        "\n",
        "    parameter_count = (\n",
        "        q_parameter_count\n",
        "        + k_parameter_count\n",
        "        + v_parameter_count\n",
        "        + o_parameter_count\n",
        "    )\n",
        "\n",
        "    return parameter_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z3vlFJXgpFzi"
      },
      "source": [
        "### Coding Activity 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z9Oh5dkdpFkV"
      },
      "outputs": [],
      "source": [
        "def parameter_count_mlp(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for the MLP component.\n",
        "\n",
        "    The MLP is modeled as two dense layers with biases:\n",
        "    - First projection: `embedding_dim -> mlp_dim`\n",
        "    - Second projection: `mlp_dim -> embedding_dim`\n",
        "\n",
        "    Args:\n",
        "      hyperparams: Model hyperparameters. Expects `\"embedding_dim\"` and\n",
        "          `\"mlp_dim\"`.\n",
        "\n",
        "    Returns:\n",
        "      Total number of trainable parameters for the MLP sublayer.\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
        "    mlp_dim = hyperparams[\"mlp_dim\"]\n",
        "\n",
        "    # Parameters for first projection component.\n",
        "    ffn_parameter_count = (embedding_dim + 1) * mlp_dim\n",
        "    # Parameters for second projection component.\n",
        "    output_parameter_count = (mlp_dim + 1) * embedding_dim\n",
        "\n",
        "    parameter_count = (\n",
        "        ffn_parameter_count + output_parameter_count\n",
        "    )\n",
        "    return parameter_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dudIwnHrpHT1"
      },
      "source": [
        "### Coding Activity 4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmYKBz2RpPQT"
      },
      "outputs": [],
      "source": [
        "def parameter_count_transformer_block(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for a transformer block (attention + MLP).\n",
        "\n",
        "    Sums the parameters from the multi-head attention component (plus its\n",
        "    LayerNorm) and the MLP component (plus its LayerNorm).\n",
        "\n",
        "    Args:\n",
        "      hyperparams : Model hyperparameters needed by the attention and MLP\n",
        "        parameter count calculators. Typically includes `\"embedding_dim\"`,\n",
        "        `\"mlp_dim\"`, and potentially others.\n",
        "\n",
        "    Returns:\n",
        "      Total number of trainable parameters for one transformer block.\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
        "\n",
        "    # Parameters for multi-head attention mechanism.\n",
        "    mha_parameter_count = parameter_count_attention(hyperparams)\n",
        "\n",
        "    # Parameters for MLP component.\n",
        "    mlp_parameter_count = parameter_count_mlp(hyperparams)\n",
        "\n",
        "    # Parameters for two layer norm components.\n",
        "    layer_norm_parameter_count = 2 * embedding_dim\n",
        "\n",
        "    parameter_count = (\n",
        "        mha_parameter_count + mlp_parameter_count + layer_norm_parameter_count\n",
        "    )\n",
        "    return parameter_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhPEWcc9pJ0r"
      },
      "source": [
        "### Coding Activity 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVwgSLhYpKRU"
      },
      "outputs": [],
      "source": [
        "def parameter_count_output_layer(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for the output projection layer.\n",
        "\n",
        "    The output projection maps from `embedding_dim` to `vocabulary_size` and\n",
        "    includes a bias term for each vocabulary entry.\n",
        "\n",
        "    Args:\n",
        "      hyperparams: Model hyperparameters. Expects `\"vocabulary_size\"` and\n",
        "        `\"embedding_dim\"`.\n",
        "\n",
        "    Returns:\n",
        "      Total number of trainable parameters for the output layer.\n",
        "    \"\"\"\n",
        "\n",
        "    embedding_dim = hyperparams[\"embedding_dim\"]\n",
        "    vocabulary_size = hyperparams[\"vocabulary_size\"]\n",
        "\n",
        "    # Parameters for output projection.\n",
        "    output_parameter_count = (embedding_dim + 1) * vocabulary_size\n",
        "\n",
        "    # Only the projection component has parameters,\n",
        "    # the activation function does not.\n",
        "    parameter_count = output_parameter_count\n",
        "\n",
        "    return parameter_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZE-2CMifpOst"
      },
      "source": [
        "### Coding Activity 6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iG5hyuJApSKQ"
      },
      "outputs": [],
      "source": [
        "def parameter_count_transformer(hyperparams: dict[str, int]) -> int:\n",
        "    \"\"\"Computes parameters for an entire transformer model consisting of\n",
        "    `num_blocks` blocks.\n",
        "\n",
        "    Args:\n",
        "      hyperparams: Model hyperparameters needed by the transformer block\n",
        "        parameter count calculator, including `\"embedding_dim\"`,\n",
        "        `\"vocabulary_size\", `\"mlp_dim\"`, `\"num_blocks\"`, and potentially others.\n",
        "\n",
        "    Returns:\n",
        "      Total number of trainable parameters for the transformer model.\n",
        "    \"\"\"\n",
        "\n",
        "    num_blocks = hyperparams[\"num_blocks\"]\n",
        "\n",
        "    # Parameter count of embedding layer.\n",
        "    embedding_parameter_count = parameter_count_embedding(hyperparams)\n",
        "\n",
        "    # Parameter count of `num_blocks` transformer blocks.\n",
        "    transformer_blocks_parameter_count = (\n",
        "        num_blocks * parameter_count_transformer_block(hyperparams)\n",
        "    )\n",
        "\n",
        "    # Parameter count of output_layer.\n",
        "    output_parameter_count = parameter_count_output_layer(hyperparams)\n",
        "\n",
        "    parameter_count = (\n",
        "        embedding_parameter_count\n",
        "        + transformer_blocks_parameter_count\n",
        "        + output_parameter_count\n",
        "    )\n",
        "\n",
        "    return parameter_count"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "96w3epfLDaKF",
        "Ss0HGXjueYhK",
        "WBLRvtNWo9AW",
        "BbNWHXOdpEMO",
        "Z3vlFJXgpFzi",
        "dudIwnHrpHT1",
        "EhPEWcc9pJ0r",
        "ZE-2CMifpOst"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
